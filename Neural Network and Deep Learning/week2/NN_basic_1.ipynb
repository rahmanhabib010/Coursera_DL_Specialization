{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part1\n",
    "Instructions: x could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices...) are called numpy arrays. You don't need to know more for now.\n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "# example of np.exp\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x)) # result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "**Exercise**: Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is: $$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "You often code this function in two steps:\n",
    "1. Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.\n",
    "2. Compute $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid_derivative\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    ds = s*(1-s)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### 1.3 - Reshaping arrays ###\n",
    "\n",
    "Two common numpy functions used in deep learning are [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) and [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- X.shape is used to get the shape (dimension) of a matrix/vector X. \n",
    "- X.reshape(...) is used to reshape X into some other dimension. \n",
    "\n",
    "For example, in computer science, an image is represented by a 3D array of shape $(length, height, depth = 3)$. However, when you read an image as the input of an algorithm you convert it to a vector of shape $(length*height*3, 1)$. In other words, you \"unroll\", or reshape, the 3D array into a 1D vector.\n",
    "\n",
    "<img src=\"images/image2vector.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "**Exercise**: Implement `image2vector()` that takes an input of shape (length, height, 3) and returns a vector of shape (length\\*height\\*3, 1). For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:\n",
    "``` python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "```\n",
    "- Please don't hardcode the dimensions of image as a constant. Instead look up the quantities you need with `image.shape[0]`, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: image2vector\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2],1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### Normalizing rows\n",
    "\n",
    "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n",
    "\n",
    "For example, if $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you're going to learn about it in part 5.\n",
    "\n",
    "\n",
    "**Exercise**: Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: normalizeRows\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    x_norm = np.linalg.norm(x,axis=1,keepdims=True)\n",
    "    \n",
    "    # Divide x by its norm.\n",
    "    x = x/x_norm\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### The softmax function ####\n",
    "**Exercise**: Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization.\n",
    "\n",
    "**Instructions**:\n",
    "- $ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  $$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (m,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    s = x_exp/x_sum\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.0ms\n",
      "outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      " ----- Computation time = 0.0ms\n",
      "elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
      " ----- Computation time = 0.0ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdot = [28.05627784 13.97434258 15.62633046]\n",
      " ----- Computation time = 0.0ms\n"
     ]
    }
   ],
   "source": [
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### Implement the L1 and L2 loss functions\n",
    "\n",
    "**Exercise**: Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.\n",
    "\n",
    "**Reminder**:\n",
    "- The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
    "- L1 loss is defined as:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L1\n",
    "\n",
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = np.sum(np.subtract(y,yhat))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 0.4999999999999999\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "**Exercise**: Implement the numpy vectorized version of the L2 loss. There are several way of implementing the L2 loss but you may find the function np.dot() useful. As a reminder, if $x = [x_1, x_2, ..., x_n]$, then `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L2\n",
    "\n",
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = np.sum((np.subtract(y,yhat))**2)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "cat_images = np.random.randn(700, 2) + np.array([0, -3])\n",
    "mouse_images = np.random.randn(700, 2) + np.array([3, 3])\n",
    "dog_images = np.random.randn(700, 2) + np.array([-3, 3])\n",
    "\n",
    "cat_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = np.vstack([cat_images, mouse_images, dog_images])\n",
    "\n",
    "labels = np.array([0]*700 + [1]*700 + [2]*700)\n",
    "\n",
    "one_hot_labels = np.zeros((2100, 3))\n",
    "\n",
    "for i in range(2100):\n",
    "    one_hot_labels[i, labels[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = np.loadtxt('E:/Data Project/hw3/data/largeTrain.csv', dtype=int, delimiter=',')\n",
    "valid_all = np.loadtxt('E:/Data Project/hw3/data/largeValidation.csv', dtype=int, delimiter=',')\n",
    "\n",
    "# print(train_all.shape)\n",
    "# print(valid_all.shape)\n",
    "\n",
    "X_train = train_all[:, 1:]\n",
    "y_train = train_all[:, 0]\n",
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "# y_train = y_train.T\n",
    "X_val = valid_all[:, 1:]\n",
    "X_val = X_val.T \n",
    "y_val = valid_all[:, 0]\n",
    "y_val = y_val.reshape(y_val.shape[0],1)\n",
    "feature_set=X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances = feature_set.shape[0]\n",
    "attributes = feature_set.shape[1]\n",
    "hidden_nodes = 5\n",
    "output_labels = 10\n",
    "instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = np.zeros((9000, 10))\n",
    "for i in range(one_hot_labels.shape[0]):\n",
    "        one_hot_labels[i,y_train[i,0]]=1\n",
    "\n",
    "# for i in range(2100):\n",
    "#     one_hot_labels[i, labels[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh = np.random.rand(attributes,hidden_nodes)\n",
    "bh = np.random.randn(hidden_nodes)\n",
    "\n",
    "wo = np.random.rand(hidden_nodes,output_labels)\n",
    "bo = np.random.randn(output_labels)\n",
    "lr = 10e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x) *(1-sigmoid (x))\n",
    "\n",
    "def softmax(A):\n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function value:  23490.545767133928\n",
      "Loss function value:  12226.159027393396\n",
      "Loss function value:  14730.897304151691\n",
      "Loss function value:  11307.393138169024\n",
      "Loss function value:  12087.097033184315\n",
      "Loss function value:  14486.173631846665\n",
      "Loss function value:  11917.64597095009\n",
      "Loss function value:  8275.944349080039\n",
      "Loss function value:  8317.98448204028\n",
      "Loss function value:  7412.964968919788\n",
      "Loss function value:  7770.527221780903\n",
      "Loss function value:  7566.466408880344\n",
      "Loss function value:  7382.1250442073815\n",
      "Loss function value:  7379.158723699338\n",
      "Loss function value:  7476.023355502302\n",
      "Loss function value:  7393.244812770317\n",
      "Loss function value:  6691.30875277113\n",
      "Loss function value:  6110.76107236342\n",
      "Loss function value:  5730.895612275265\n",
      "Loss function value:  5696.56021063924\n",
      "Loss function value:  5537.566938023705\n",
      "Loss function value:  5793.110919994653\n",
      "Loss function value:  5426.9863716369255\n",
      "Loss function value:  5464.634659084977\n",
      "Loss function value:  5224.583472601009\n",
      "Loss function value:  5360.5953550553495\n",
      "Loss function value:  5215.196309859757\n",
      "Loss function value:  5190.5529498916885\n",
      "Loss function value:  5164.97097880738\n",
      "Loss function value:  5143.857137652527\n",
      "Loss function value:  5126.71893430832\n",
      "Loss function value:  5113.973135506246\n",
      "Loss function value:  5103.565060452498\n",
      "Loss function value:  5095.394139844166\n",
      "Loss function value:  5088.631488853881\n",
      "Loss function value:  5081.25778097959\n",
      "Loss function value:  5073.9516892989195\n",
      "Loss function value:  5066.662383582107\n",
      "Loss function value:  5057.43511664106\n",
      "Loss function value:  5048.982771986516\n",
      "Loss function value:  5041.9641596686615\n",
      "Loss function value:  5035.445934219892\n",
      "Loss function value:  5028.649444733741\n",
      "Loss function value:  5021.523390116769\n",
      "Loss function value:  5014.257516559073\n",
      "Loss function value:  5006.85395944878\n",
      "Loss function value:  4999.344419874245\n",
      "Loss function value:  4992.157152626728\n",
      "Loss function value:  4985.552929915184\n",
      "Loss function value:  4979.711952973826\n",
      "Loss function value:  4974.935010067243\n",
      "Loss function value:  4972.019674823739\n",
      "Loss function value:  4967.765225964316\n",
      "Loss function value:  4961.962952087359\n",
      "Loss function value:  4956.58679756017\n",
      "Loss function value:  4951.768722719953\n",
      "Loss function value:  4947.397205374893\n",
      "Loss function value:  4943.5406998917015\n",
      "Loss function value:  4940.781088945862\n",
      "Loss function value:  4938.171881266266\n",
      "Loss function value:  4935.381794562064\n",
      "Loss function value:  4932.721319743112\n",
      "Loss function value:  4930.178275831625\n",
      "Loss function value:  4927.7116049695605\n",
      "Loss function value:  4925.283747525543\n",
      "Loss function value:  4922.870578011414\n",
      "Loss function value:  4920.576151578962\n",
      "Loss function value:  4918.633321342582\n",
      "Loss function value:  4916.777395942956\n",
      "Loss function value:  4914.900970075107\n",
      "Loss function value:  4912.5620137504575\n",
      "Loss function value:  4909.16295663307\n",
      "Loss function value:  4906.69963536694\n",
      "Loss function value:  4903.3903639941645\n",
      "Loss function value:  4900.807965709297\n",
      "Loss function value:  4898.720673785174\n",
      "Loss function value:  4896.93042791927\n",
      "Loss function value:  4895.282463716858\n",
      "Loss function value:  4893.668999455329\n",
      "Loss function value:  4892.01682270956\n",
      "Loss function value:  4890.296632680047\n",
      "Loss function value:  4888.530569141025\n",
      "Loss function value:  4886.778894489898\n",
      "Loss function value:  4885.1331758120805\n",
      "Loss function value:  4883.7416730378145\n",
      "Loss function value:  4882.491615906001\n",
      "Loss function value:  4880.286958818857\n",
      "Loss function value:  4877.844055160134\n",
      "Loss function value:  4875.725883033635\n",
      "Loss function value:  4873.977525705335\n",
      "Loss function value:  4872.413895139991\n",
      "Loss function value:  4870.669594483665\n",
      "Loss function value:  4868.441147842855\n",
      "Loss function value:  4865.894926790635\n",
      "Loss function value:  4863.293038373092\n",
      "Loss function value:  4860.788803881281\n",
      "Loss function value:  4858.704982610948\n",
      "Loss function value:  4857.411307365349\n",
      "Loss function value:  4856.312543214444\n",
      "Loss function value:  4854.983096316978\n",
      "Loss function value:  4853.532932183934\n",
      "Loss function value:  4852.053356937411\n",
      "Loss function value:  4850.584846099211\n",
      "Loss function value:  4849.14398152965\n",
      "Loss function value:  4847.735306734623\n",
      "Loss function value:  4846.357154464857\n",
      "Loss function value:  4845.005429323675\n",
      "Loss function value:  4843.676375055255\n",
      "Loss function value:  4842.368378399057\n",
      "Loss function value:  4841.082797398939\n",
      "Loss function value:  4839.824434196026\n",
      "Loss function value:  4838.602992562029\n",
      "Loss function value:  4837.437436221654\n",
      "Loss function value:  4836.365286976187\n",
      "Loss function value:  4835.452846490556\n",
      "Loss function value:  4834.776851108766\n",
      "Loss function value:  4834.36228655705\n",
      "Loss function value:  4834.244788756529\n",
      "Loss function value:  4834.413893065004\n",
      "Loss function value:  4834.069812504804\n",
      "Loss function value:  4833.02848564695\n",
      "Loss function value:  4831.731850442405\n",
      "Loss function value:  4830.385774753589\n",
      "Loss function value:  4829.050701547929\n",
      "Loss function value:  4827.736463742413\n",
      "Loss function value:  4826.428695879731\n",
      "Loss function value:  4825.078610848839\n",
      "Loss function value:  4823.443207396584\n",
      "Loss function value:  4844.474287850998\n",
      "Loss function value:  4832.28655798039\n",
      "Loss function value:  4829.748184926593\n",
      "Loss function value:  4828.1387162821375\n",
      "Loss function value:  4826.866292423907\n",
      "Loss function value:  4825.770272894851\n",
      "Loss function value:  4824.786831763087\n",
      "Loss function value:  4823.884211241025\n",
      "Loss function value:  4823.043981253754\n",
      "Loss function value:  4822.254220438773\n",
      "Loss function value:  4821.506594133957\n",
      "Loss function value:  4820.794934666125\n",
      "Loss function value:  4820.114480125924\n",
      "Loss function value:  4819.461433299702\n",
      "Loss function value:  4818.8326899571675\n",
      "Loss function value:  4818.2256630170195\n",
      "Loss function value:  4817.638163990807\n",
      "Loss function value:  4817.068320121229\n",
      "Loss function value:  4816.514514526736\n",
      "Loss function value:  4815.975341602136\n",
      "Loss function value:  4815.449572804868\n",
      "Loss function value:  4814.936129701433\n",
      "Loss function value:  4814.434062233545\n",
      "Loss function value:  4813.942530849424\n",
      "Loss function value:  4813.460791583106\n",
      "Loss function value:  4812.988183445352\n",
      "Loss function value:  4812.524117671199\n",
      "Loss function value:  4812.068068488241\n",
      "Loss function value:  4811.619565149495\n",
      "Loss function value:  4811.178185029964\n",
      "Loss function value:  4810.743547625853\n",
      "Loss function value:  4810.3153093251785\n",
      "Loss function value:  4809.893158841528\n",
      "Loss function value:  4809.47681322107\n",
      "Loss function value:  4809.066014347527\n",
      "Loss function value:  4808.660525881758\n",
      "Loss function value:  4808.260130582252\n",
      "Loss function value:  4807.864627960691\n",
      "Loss function value:  4807.473832233109\n",
      "Loss function value:  4807.087570532435\n",
      "Loss function value:  4806.705681352439\n",
      "Loss function value:  4806.328013196679\n",
      "Loss function value:  4805.954423408991\n",
      "Loss function value:  4805.584777164532\n",
      "Loss function value:  4805.218946602659\n",
      "Loss function value:  4804.856810084701\n",
      "Loss function value:  4804.49825156145\n",
      "Loss function value:  4804.143160036697\n",
      "Loss function value:  4803.791429114539\n",
      "Loss function value:  4803.442956619399\n",
      "Loss function value:  4803.0976442789215\n",
      "Loss function value:  4802.75539746084\n",
      "Loss function value:  4802.416124955846\n",
      "Loss function value:  4802.079738799206\n",
      "Loss function value:  4801.7461541244365\n",
      "Loss function value:  4801.415289042756\n",
      "Loss function value:  4801.087064542303\n",
      "Loss function value:  4800.7614044012125\n",
      "Loss function value:  4800.438235108691\n",
      "Loss function value:  4800.117485788292\n",
      "Loss function value:  4799.799088117731\n",
      "Loss function value:  4799.482976239862\n",
      "Loss function value:  4799.169086660174\n",
      "Loss function value:  4798.857358127142\n",
      "Loss function value:  4798.547731493305\n",
      "Loss function value:  4798.24014955688\n",
      "Loss function value:  4797.934556886064\n",
      "Loss function value:  4797.630899630751\n",
      "Loss function value:  4797.329125328969\n",
      "Loss function value:  4797.029182717779\n",
      "Loss function value:  4796.731021560264\n",
      "Loss function value:  4796.434592501545\n",
      "Loss function value:  4796.139846967062\n",
      "Loss function value:  4795.846737115668\n",
      "Loss function value:  4795.55521585819\n",
      "Loss function value:  4795.265236948944\n",
      "Loss function value:  4794.976755152925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function value:  4794.689726485203\n",
      "Loss function value:  4794.404108510998\n",
      "Loss function value:  4794.119860685292\n",
      "Loss function value:  4793.836944700432\n",
      "Loss function value:  4793.555324800697\n",
      "Loss function value:  4793.274968018312\n",
      "Loss function value:  4792.995844292438\n",
      "Loss function value:  4792.717926462213\n",
      "Loss function value:  4792.441190190781\n",
      "Loss function value:  4792.165613994376\n",
      "Loss function value:  4791.891179724449\n",
      "Loss function value:  4791.617874058122\n",
      "Loss function value:  4791.345691708478\n",
      "Loss function value:  4791.074640994325\n",
      "Loss function value:  4790.8047518533385\n",
      "Loss function value:  4790.536085145875\n",
      "Loss function value:  4790.268740350593\n",
      "Loss function value:  4790.002857351827\n",
      "Loss function value:  4789.738608345518\n",
      "Loss function value:  4789.4761788285605\n",
      "Loss function value:  4789.215741263557\n",
      "Loss function value:  4788.957428629557\n",
      "Loss function value:  4788.70131524417\n",
      "Loss function value:  4788.447408963573\n",
      "Loss function value:  4788.1956544242785\n",
      "Loss function value:  4787.945943858598\n",
      "Loss function value:  4787.698131152655\n",
      "Loss function value:  4787.452045677374\n",
      "Loss function value:  4787.207503951306\n",
      "Loss function value:  4786.964318604358\n",
      "Loss function value:  4786.722305087629\n",
      "Loss function value:  4786.481287108876\n",
      "Loss function value:  4786.241101949596\n",
      "Loss function value:  4786.001606678361\n",
      "Loss function value:  4785.762685784296\n",
      "Loss function value:  4785.524259883629\n",
      "Loss function value:  4785.286294027035\n",
      "Loss function value:  4785.04880317505\n",
      "Loss function value:  4784.811852252219\n",
      "Loss function value:  4784.575549288257\n",
      "Loss function value:  4784.34003225796\n",
      "Loss function value:  4784.1054523347075\n",
      "Loss function value:  4783.87195725622\n",
      "Loss function value:  4783.639677957889\n",
      "Loss function value:  4783.408720072648\n"
     ]
    }
   ],
   "source": [
    "error_cost = []\n",
    "\n",
    "for epoch in range(50000):\n",
    "############# feedforward\n",
    "\n",
    "    # Phase 1\n",
    "    zh = np.dot(feature_set, wh) + bh\n",
    "    ah = sigmoid(zh)\n",
    "\n",
    "    # Phase 2\n",
    "    zo = np.dot(ah, wo) + bo\n",
    "    ao = softmax(zo)\n",
    "\n",
    "########## Back Propagation\n",
    "\n",
    "########## Phase 1\n",
    "\n",
    "    dcost_dzo = ao - one_hot_labels\n",
    "    dzo_dwo = ah\n",
    "\n",
    "    dcost_wo = np.dot(dzo_dwo.T, dcost_dzo)\n",
    "\n",
    "    dcost_bo = dcost_dzo\n",
    "\n",
    "########## Phases 2\n",
    "\n",
    "    dzo_dah = wo\n",
    "    dcost_dah = np.dot(dcost_dzo , dzo_dah.T)\n",
    "    dah_dzh = sigmoid_der(zh)\n",
    "    dzh_dwh = feature_set\n",
    "    dcost_wh = np.dot(dzh_dwh.T, dah_dzh * dcost_dah)\n",
    "\n",
    "    dcost_bh = dcost_dah * dah_dzh\n",
    "\n",
    "    # Update Weights ================\n",
    "\n",
    "    wh -= lr * dcost_wh\n",
    "    bh -= lr * dcost_bh.sum(axis=0)\n",
    "\n",
    "    wo -= lr * dcost_wo\n",
    "    bo -= lr * dcost_bo.sum(axis=0)\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        loss = np.sum(-one_hot_labels * np.log(ao))\n",
    "        print('Loss function value: ', loss)\n",
    "        error_cost.append(loss)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "error_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23490.545767133928,\n",
       " 12226.159027393396,\n",
       " 14730.897304151691,\n",
       " 11307.393138169024,\n",
       " 12087.097033184315,\n",
       " 14486.173631846665,\n",
       " 11917.64597095009,\n",
       " 8275.944349080039,\n",
       " 8317.98448204028,\n",
       " 7412.964968919788,\n",
       " 7770.527221780903,\n",
       " 7566.466408880344,\n",
       " 7382.1250442073815,\n",
       " 7379.158723699338,\n",
       " 7476.023355502302,\n",
       " 7393.244812770317,\n",
       " 6691.30875277113,\n",
       " 6110.76107236342,\n",
       " 5730.895612275265,\n",
       " 5696.56021063924,\n",
       " 5537.566938023705,\n",
       " 5793.110919994653,\n",
       " 5426.9863716369255,\n",
       " 5464.634659084977,\n",
       " 5224.583472601009,\n",
       " 5360.5953550553495,\n",
       " 5215.196309859757,\n",
       " 5190.5529498916885,\n",
       " 5164.97097880738,\n",
       " 5143.857137652527,\n",
       " 5126.71893430832,\n",
       " 5113.973135506246,\n",
       " 5103.565060452498,\n",
       " 5095.394139844166,\n",
       " 5088.631488853881,\n",
       " 5081.25778097959,\n",
       " 5073.9516892989195,\n",
       " 5066.662383582107,\n",
       " 5057.43511664106,\n",
       " 5048.982771986516,\n",
       " 5041.9641596686615,\n",
       " 5035.445934219892,\n",
       " 5028.649444733741,\n",
       " 5021.523390116769,\n",
       " 5014.257516559073,\n",
       " 5006.85395944878,\n",
       " 4999.344419874245,\n",
       " 4992.157152626728,\n",
       " 4985.552929915184,\n",
       " 4979.711952973826,\n",
       " 4974.935010067243,\n",
       " 4972.019674823739,\n",
       " 4967.765225964316,\n",
       " 4961.962952087359,\n",
       " 4956.58679756017,\n",
       " 4951.768722719953,\n",
       " 4947.397205374893,\n",
       " 4943.5406998917015,\n",
       " 4940.781088945862,\n",
       " 4938.171881266266,\n",
       " 4935.381794562064,\n",
       " 4932.721319743112,\n",
       " 4930.178275831625,\n",
       " 4927.7116049695605,\n",
       " 4925.283747525543,\n",
       " 4922.870578011414,\n",
       " 4920.576151578962,\n",
       " 4918.633321342582,\n",
       " 4916.777395942956,\n",
       " 4914.900970075107,\n",
       " 4912.5620137504575,\n",
       " 4909.16295663307,\n",
       " 4906.69963536694,\n",
       " 4903.3903639941645,\n",
       " 4900.807965709297,\n",
       " 4898.720673785174,\n",
       " 4896.93042791927,\n",
       " 4895.282463716858,\n",
       " 4893.668999455329,\n",
       " 4892.01682270956,\n",
       " 4890.296632680047,\n",
       " 4888.530569141025,\n",
       " 4886.778894489898,\n",
       " 4885.1331758120805,\n",
       " 4883.7416730378145,\n",
       " 4882.491615906001,\n",
       " 4880.286958818857,\n",
       " 4877.844055160134,\n",
       " 4875.725883033635,\n",
       " 4873.977525705335,\n",
       " 4872.413895139991,\n",
       " 4870.669594483665,\n",
       " 4868.441147842855,\n",
       " 4865.894926790635,\n",
       " 4863.293038373092,\n",
       " 4860.788803881281,\n",
       " 4858.704982610948,\n",
       " 4857.411307365349,\n",
       " 4856.312543214444,\n",
       " 4854.983096316978,\n",
       " 4853.532932183934,\n",
       " 4852.053356937411,\n",
       " 4850.584846099211,\n",
       " 4849.14398152965,\n",
       " 4847.735306734623,\n",
       " 4846.357154464857,\n",
       " 4845.005429323675,\n",
       " 4843.676375055255,\n",
       " 4842.368378399057,\n",
       " 4841.082797398939,\n",
       " 4839.824434196026,\n",
       " 4838.602992562029,\n",
       " 4837.437436221654,\n",
       " 4836.365286976187,\n",
       " 4835.452846490556,\n",
       " 4834.776851108766,\n",
       " 4834.36228655705,\n",
       " 4834.244788756529,\n",
       " 4834.413893065004,\n",
       " 4834.069812504804,\n",
       " 4833.02848564695,\n",
       " 4831.731850442405,\n",
       " 4830.385774753589,\n",
       " 4829.050701547929,\n",
       " 4827.736463742413,\n",
       " 4826.428695879731,\n",
       " 4825.078610848839,\n",
       " 4823.443207396584,\n",
       " 4844.474287850998,\n",
       " 4832.28655798039,\n",
       " 4829.748184926593,\n",
       " 4828.1387162821375,\n",
       " 4826.866292423907,\n",
       " 4825.770272894851,\n",
       " 4824.786831763087,\n",
       " 4823.884211241025,\n",
       " 4823.043981253754,\n",
       " 4822.254220438773,\n",
       " 4821.506594133957,\n",
       " 4820.794934666125,\n",
       " 4820.114480125924,\n",
       " 4819.461433299702,\n",
       " 4818.8326899571675,\n",
       " 4818.2256630170195,\n",
       " 4817.638163990807,\n",
       " 4817.068320121229,\n",
       " 4816.514514526736,\n",
       " 4815.975341602136,\n",
       " 4815.449572804868,\n",
       " 4814.936129701433,\n",
       " 4814.434062233545,\n",
       " 4813.942530849424,\n",
       " 4813.460791583106,\n",
       " 4812.988183445352,\n",
       " 4812.524117671199,\n",
       " 4812.068068488241,\n",
       " 4811.619565149495,\n",
       " 4811.178185029964,\n",
       " 4810.743547625853,\n",
       " 4810.3153093251785,\n",
       " 4809.893158841528,\n",
       " 4809.47681322107,\n",
       " 4809.066014347527,\n",
       " 4808.660525881758,\n",
       " 4808.260130582252,\n",
       " 4807.864627960691,\n",
       " 4807.473832233109,\n",
       " 4807.087570532435,\n",
       " 4806.705681352439,\n",
       " 4806.328013196679,\n",
       " 4805.954423408991,\n",
       " 4805.584777164532,\n",
       " 4805.218946602659,\n",
       " 4804.856810084701,\n",
       " 4804.49825156145,\n",
       " 4804.143160036697,\n",
       " 4803.791429114539,\n",
       " 4803.442956619399,\n",
       " 4803.0976442789215,\n",
       " 4802.75539746084,\n",
       " 4802.416124955846,\n",
       " 4802.079738799206,\n",
       " 4801.7461541244365,\n",
       " 4801.415289042756,\n",
       " 4801.087064542303,\n",
       " 4800.7614044012125,\n",
       " 4800.438235108691,\n",
       " 4800.117485788292,\n",
       " 4799.799088117731,\n",
       " 4799.482976239862,\n",
       " 4799.169086660174,\n",
       " 4798.857358127142,\n",
       " 4798.547731493305,\n",
       " 4798.24014955688,\n",
       " 4797.934556886064,\n",
       " 4797.630899630751,\n",
       " 4797.329125328969,\n",
       " 4797.029182717779,\n",
       " 4796.731021560264,\n",
       " 4796.434592501545,\n",
       " 4796.139846967062,\n",
       " 4795.846737115668,\n",
       " 4795.55521585819,\n",
       " 4795.265236948944,\n",
       " 4794.976755152925,\n",
       " 4794.689726485203,\n",
       " 4794.404108510998,\n",
       " 4794.119860685292,\n",
       " 4793.836944700432,\n",
       " 4793.555324800697,\n",
       " 4793.274968018312,\n",
       " 4792.995844292438,\n",
       " 4792.717926462213,\n",
       " 4792.441190190781,\n",
       " 4792.165613994376,\n",
       " 4791.891179724449,\n",
       " 4791.617874058122,\n",
       " 4791.345691708478,\n",
       " 4791.074640994325,\n",
       " 4790.8047518533385,\n",
       " 4790.536085145875,\n",
       " 4790.268740350593,\n",
       " 4790.002857351827,\n",
       " 4789.738608345518,\n",
       " 4789.4761788285605,\n",
       " 4789.215741263557,\n",
       " 4788.957428629557,\n",
       " 4788.70131524417,\n",
       " 4788.447408963573,\n",
       " 4788.1956544242785,\n",
       " 4787.945943858598,\n",
       " 4787.698131152655,\n",
       " 4787.452045677374,\n",
       " 4787.207503951306,\n",
       " 4786.964318604358,\n",
       " 4786.722305087629,\n",
       " 4786.481287108876,\n",
       " 4786.241101949596,\n",
       " 4786.001606678361,\n",
       " 4785.762685784296,\n",
       " 4785.524259883629,\n",
       " 4785.286294027035,\n",
       " 4785.04880317505,\n",
       " 4784.811852252219,\n",
       " 4784.575549288257,\n",
       " 4784.34003225796,\n",
       " 4784.1054523347075,\n",
       " 4783.87195725622,\n",
       " 4783.639677957889,\n",
       " 4783.408720072648]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-e938b1ef9acd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0merror_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "m=9000\n",
    "plt.plot(1/m*error_cost)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
